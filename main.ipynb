{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import imageio\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int, action_size: int):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden_size = 32\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "        self.layers = self.layers.double() #  set network param to double\n",
    "        \n",
    "    def forward(self, state):\n",
    "        q_values = self.layers(state.double())\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublePendulumAgent():\n",
    "    def __init__(self, space_dim, action_dim):\n",
    "        self.TAU = 1e-3  # Soft update parameter.\n",
    "        self.ALPHA = 1e-3 #learning rate\n",
    "        self.MINIBATCH_SIZE = 64\n",
    "        self.GAMMA = 0.995\n",
    "        self.epsilon = 1.0\n",
    "        self.E_MIN = 0.01\n",
    "        self.network = QNetwork(space_dim, action_dim)\n",
    "        self.target_network = QNetwork(space_dim, action_dim)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.network.parameters(),\n",
    "            lr=self.ALPHA)\n",
    "        \n",
    "    def get_action(self, q_values, env):\n",
    "        if random.random() > self.epsilon:\n",
    "            next_action = q_values.detach().numpy()[0]\n",
    "            print(\" ekki random next action : \", next_action)\n",
    "            print(\"inni if shape: \", next_action.shape)\n",
    "            return np.argmax(next_action)\n",
    "        else:\n",
    "            next_action = np.array([np.random.uniform(-3, 4)])\n",
    "            print(\"next action : \",next_action)\n",
    "            print(\"ekki if shape: \", next_action.shape)\n",
    "\n",
    "            return next_action\n",
    "                     \n",
    "        # next_action = q_values.detach().numpy()[0]\n",
    "        # return next_action\n",
    "        # return np.argmax(q_values.detach().numpy())\n",
    "        \n",
    "    def should_update_network(self, num_episode, num_steps_upd, memory_buffer):\n",
    "        return num_episode % num_steps_upd == 0 and len(memory_buffer) > self.MINIBATCH_SIZE\n",
    "    \n",
    "    def agent_learn(self, experiences):\n",
    "        \"\"\"\n",
    "        Updates the weights of the Q networks.\n",
    "        Args:\n",
    "        experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "        gamma: (float) The discount factor.\n",
    "        q_network: PyTorch model for the Q-network.\n",
    "        target_q_network: PyTorch model for the target Q-network.\n",
    "        optimizer: PyTorch optimizer.\n",
    "        \"\"\"\n",
    "        # Zero the gradients before backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.compute_loss(experiences)\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights of the q_network\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update the weights of target q_network\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def get_experiences(self, memory_buffer):\n",
    "        experiences = random.sample(memory_buffer, k=self.MINIBATCH_SIZE)\n",
    "        states = torch.tensor(\n",
    "            [e.state for e in experiences if e is not None], dtype=torch.double\n",
    "        )\n",
    "        actions = torch.tensor(\n",
    "            [e.action for e in experiences if e is not None], dtype=torch.double\n",
    "        )\n",
    "        rewards = torch.tensor(\n",
    "            [e.reward for e in experiences if e is not None], dtype=torch.double\n",
    "        )\n",
    "        next_states = torch.tensor(\n",
    "            [e.next_state for e in experiences if e is not None], dtype=torch.double\n",
    "        )\n",
    "        done_vals = torch.tensor(\n",
    "            [e.done for e in experiences if e is not None], dtype=torch.uint8\n",
    "        ).float()  # Convert to float after creating the tensor TODO: check if this is correct\n",
    "\n",
    "        return states, actions, rewards, next_states, done_vals\n",
    "\n",
    "    \n",
    "    def compute_loss(self, experiences):\n",
    "        \"\"\"\n",
    "        Calculates the loss.\n",
    "\n",
    "        Args:\n",
    "        experiences: namedtuple with fields [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "        gamma: discount factor.\n",
    "        q_network: PyTorch model for predicting the q_values.\n",
    "        target_q_network: PyTorch model for predicting the targets.\n",
    "\n",
    "        Returns:\n",
    "        loss: Mean-Squared Error between the y targets and the Q(s,a) values.\n",
    "        \"\"\"\n",
    "        # Unpack the mini-batch of experience tuples\n",
    "        states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "        # Compute max Q^(s,a) using target network\n",
    "        with torch.no_grad():  # No gradient computation for target network\n",
    "            max_qsa = self.target_network(next_states).max(1)[0]  # max function returns both values and indices\n",
    "\n",
    "        # Set y = R if episode terminates, otherwise set y = R + Î³ max Q^(s,a)\n",
    "        y_targets = rewards + (self.GAMMA * max_qsa * (1 - done_vals))\n",
    "\n",
    "        # Get the q_values for the actions taken\n",
    "        q_values = self.network(next_states).max(1)[0]\n",
    "\n",
    "        # Compute the loss (Mean Squared Error)\n",
    "        loss = torch.nn.functional.mse_loss(q_values, y_targets)\n",
    "        \n",
    "        # # Generate a batch of data\n",
    "        # states, actions, rewards, next_states, done_vals = experiences\n",
    "        # # Generate a batch of data\n",
    "        # states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "        # max_qsa = torch.max(self.target_network(next_states), dim=1)[0]\n",
    "        # y_targets = rewards + (self.GAMMA * max_qsa * (1 - done_vals))\n",
    "        # q_values = self.network(states)\n",
    "        # loss = torch.nn.functional.binary_cross_entropy_with_logits(q_values, y_targets.unsqueeze(1))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def update_epsilon(self, num_episodes):\n",
    "        \n",
    "        self.epsilon = max(self.E_MIN, self.epsilon * ( self.epsilon / (num_episodes / 2)))\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for target_param, q_net_param in zip(self.target_network.parameters(), self.network.parameters()):\n",
    "            target_param.data.copy_(self.TAU * q_net_param.data + (1.0 - self.TAU) * target_param.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next action :  [-2.17301732]\n",
      "ekki if shape:  (1,)\n",
      "next action :  [2.42176495]\n",
      "ekki if shape:  (1,)\n",
      "next action :  [-0.42929438]\n",
      "ekki if shape:  (1,)\n",
      "next action :  [-0.37099673]\n",
      "ekki if shape:  (1,)\n",
      "next action :  [1.11454577]\n",
      "ekki if shape:  (1,)\n",
      "next action :  [-1.16885232]\n",
      "ekki if shape:  (1,)\n",
      "Episode 1 | Total point average of the last 100 episodes: 54.49: epsilon = 0.3333333333333333 ekki random next action :  [-0.1735309]\n",
      "inni if shape:  (1,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Action dimension mismatch. Expected (1,), found ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m q_values \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mnetwork(state_qn)\n\u001b[1;32m     26\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(q_values, env)\n\u001b[0;32m---> 27\u001b[0m next_state, reward, next_done, info, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m done \u001b[38;5;241m=\u001b[39m next_done\n\u001b[1;32m     29\u001b[0m memory_buffer\u001b[38;5;241m.\u001b[39mappend(experience(state, action, reward, next_state, done))\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/gymnasium/envs/mujoco/inverted_double_pendulum_v4.py:149\u001b[0m, in \u001b[0;36mInvertedDoublePendulumEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_skip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     ob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_obs()\n\u001b[1;32m    151\u001b[0m     x, _, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msite_xpos[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_env.py:148\u001b[0m, in \u001b[0;36mBaseMujocoEnv.do_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Check control input is contained in the action space\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(ctrl)\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction dimension mismatch. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39marray(ctrl)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_mujoco_simulation(ctrl, n_frames)\n",
      "\u001b[0;31mValueError\u001b[0m: Action dimension mismatch. Expected (1,), found ()"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "MAX_NUM_STEPS = 1000 #terminates after 1000 steps\n",
    "MAX_NUM_EPISODES = 40000 \n",
    "NUM_P_AVG = 100\n",
    "UPDATE_NETWORK_STEPS = 7\n",
    "\n",
    "env = gym.make('InvertedDoublePendulum-v4', render_mode=\"rgb_array\")\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "NUM_ACTIONS = env.action_space.shape[0]\n",
    "\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "memory_buffer = deque(maxlen=10000)\n",
    "total_points_hist = []\n",
    "agent = DoublePendulumAgent(STATE_SIZE, NUM_ACTIONS)\n",
    "\n",
    "for i in range(MAX_NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_points = 0\n",
    "    # for t in range(MAX_NUM_STEPS):\n",
    "    done = False\n",
    "    t = 0\n",
    "    \n",
    "    while not done:\n",
    "        t+=1\n",
    "        state_qn = torch.from_numpy(np.expand_dims(state, axis=0))\n",
    "        q_values = agent.network(state_qn)\n",
    "        action = agent.get_action(q_values, env)\n",
    "        next_state, reward, next_done, info, _ = env.step(action)\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        if(agent.should_update_network(t, UPDATE_NETWORK_STEPS, memory_buffer)):\n",
    "            minibatch = agent.get_experiences(memory_buffer)\n",
    "            agent.agent_learn(minibatch)\n",
    "            \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "    total_points_hist.append(total_points)\n",
    "    agent.update_epsilon(t)\n",
    "    \n",
    "    avg_latest_points = np.mean(total_points_hist[-NUM_P_AVG:])\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {NUM_P_AVG} episodes: {avg_latest_points:.2f}: epsilon = {agent.epsilon}\", end=\"\")\n",
    "\n",
    "    if (i+1) % NUM_P_AVG == 0:\n",
    "            print(f\"\\rEpisode {i+1} | Total point average of the last {NUM_P_AVG} episodes: {avg_latest_points:.2f}\")\n",
    "            \n",
    "    if avg_latest_points >= 300:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        torch.save(agent.network.state_dict(), 'Cart_pole_model.pth')\n",
    "        break\n",
    "    \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(filename, env, q_network, fps=30):\n",
    "    start = time.time()\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        frame = env.render()\n",
    "        video.append_data(frame)\n",
    "        while not done:\n",
    "            if time.time() - start > 10:\n",
    "                break\n",
    "            state = torch.from_numpy(np.expand_dims(state, axis=0))\n",
    "            q_values = q_network(state)\n",
    "            action = np.argmax(q_values.detach().numpy()[0])\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "            frame = env.render()\n",
    "            video.append_data(frame)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        ...,\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78]],\n",
       "\n",
       "       [[78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        ...,\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78]],\n",
       "\n",
       "       [[78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        ...,\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        ...,\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78]],\n",
       "\n",
       "       [[78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        ...,\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78]],\n",
       "\n",
       "       [[78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        ...,\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78],\n",
       "        [78, 88, 78]]], dtype=uint8)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create_video(\"double_pendulum.mp4\", env, agent.network)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
