{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import imageio\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int, action_size: int):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.hidden_size = 32\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, action_size)\n",
    "        )\n",
    "        \n",
    "        self.layers = self.layers.double() #  set network param to double\n",
    "        \n",
    "    def forward(self, state):\n",
    "        q_values = self.layers(state)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublePendulumAgent():\n",
    "    def __init__(self, space_dim, action_dim):\n",
    "        self.TAU = 1e-3  # Soft update parameter.\n",
    "        self.ALPHA = 1e-3 #learning rate\n",
    "        self.MINIBATCH_SIZE = 64\n",
    "        self.GAMMA = 0.995\n",
    "        self.epsilon = 1.0\n",
    "        self.E_MIN = 0.01\n",
    "        self.network = QNetwork(space_dim, action_dim)\n",
    "        self.target_network = QNetwork(space_dim, action_dim)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.network.parameters(),\n",
    "            lr=self.ALPHA)\n",
    "        \n",
    "    def get_action(self, q_values):\n",
    "        if random.random() > self.epsilon:\n",
    "            next_action = q_values.detach().numpy()[0]\n",
    "            return np.array([np.argmax(next_action)])  # Wrap the result in a numpy array\n",
    "        else:\n",
    "            next_action = np.array([np.random.uniform(-3, 3)])\n",
    "            return next_action\n",
    "        # if random.random() > self.epsilon:\n",
    "        #     next_action = q_values.detach().numpy()\n",
    "        #     # print(\" ekki random next action : \", next_action)\n",
    "        #     # print(\"inni if shape: \", next_action.shape)\n",
    "        #     return np.argmax(next_action)\n",
    "        # else:\n",
    "        #     next_action = np.array([np.random.uniform(-3, 3)])\n",
    "        #     # print(\"next action : \",next_action)\n",
    "        #     # print(\"ekki if shape: \", next_action.shape)\n",
    "\n",
    "        #     return next_action\n",
    "                     \n",
    "        # next_action = q_values.detach().numpy()[0]\n",
    "        # return next_action\n",
    "        # return np.argmax(q_values.detach().numpy())\n",
    "        \n",
    "    def should_update_network(self, num_episode, num_steps_upd, memory_buffer):\n",
    "        return num_episode % num_steps_upd == 0 and len(memory_buffer) > self.MINIBATCH_SIZE\n",
    "    \n",
    "    # def agent_learn(self, experiences):\n",
    "    #     \"\"\"\n",
    "    #     Updates the weights of the Q networks.\n",
    "    #     Args:\n",
    "    #     experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "    #     gamma: (float) The discount factor.\n",
    "    #     q_network: PyTorch model for the Q-network.\n",
    "    #     target_q_network: PyTorch model for the target Q-network.\n",
    "    #     optimizer: PyTorch optimizer.\n",
    "    #     \"\"\"\n",
    "    #     # Zero the gradients before backpropagation\n",
    "    #     self.optimizer.zero_grad()\n",
    "\n",
    "    #     # Calculate the loss\n",
    "    #     loss = self.compute_loss(experiences)\n",
    "\n",
    "    #     # Backpropagate the loss\n",
    "    #     loss.backward()\n",
    "\n",
    "    #     # Update the weights of the q_network\n",
    "    #     self.optimizer.step()\n",
    "\n",
    "    #     # Update the weights of target q_network\n",
    "    #     self.update_target_network()\n",
    "        \n",
    "    def get_experiences(self, memory_buffer):\n",
    "        experiences = random.sample(memory_buffer, k=self.MINIBATCH_SIZE)\n",
    "        states = torch.tensor(\n",
    "            [e.state for e in experiences if e is not None], dtype=torch.double\n",
    "        )\n",
    "        actions = torch.tensor(\n",
    "            [e.action for e in experiences if e is not None], dtype=torch.double\n",
    "        )\n",
    "        rewards = torch.tensor(\n",
    "            [e.reward for e in experiences if e is not None], dtype=torch.double\n",
    "        )\n",
    "        next_states = torch.tensor(\n",
    "            [e.next_state for e in experiences if e is not None], dtype=torch.double\n",
    "        )\n",
    "        done_vals = torch.tensor(\n",
    "            [e.done for e in experiences if e is not None], dtype=torch.uint8\n",
    "        ).float()  # Convert to float after creating the tensor TODO: check if this is correct\n",
    "\n",
    "        return states, actions, rewards, next_states, done_vals\n",
    "\n",
    "    # def compute_loss(self, experiences):\n",
    "    #     # Unpack the mini-batch of experience tuples\n",
    "    #     states, actions, rewards, next_states, done_vals = experiences\n",
    "    #     print(\" actions shape: \", actions.shape)\n",
    "    #     print(\" states shape: \", states.shape)\n",
    "    #     print(\" next_states shape: \", next_states.shape)\n",
    "\n",
    "    #     # Compute max Q^(s,a) using target network\n",
    "    #     with torch.no_grad():  # No gradient computation for target network\n",
    "    #         max_qsa = self.target_network(next_states).max(1)[0]  # max function returns both values and indices\n",
    "\n",
    "    #     # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a)\n",
    "    #     y_targets = rewards + (self.GAMMA * max_qsa * (1 - done_vals))\n",
    "\n",
    "    #     # Get the q_values for the actions taken\n",
    "    #     q_values = self.network(states).gather(1, actions.long().unsqueeze(1)).squeeze(1)\n",
    "    #     # q_values = self.network(states.transpose())\n",
    "    #     # Compute the loss (Mean Squared Error)\n",
    "    #     loss = torch.nn.functional.mse_loss(q_values, y_targets)\n",
    "\n",
    "    #     return loss\n",
    "    \n",
    "    def agent_learn(self, experiences):\n",
    "        # Zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.compute_loss(experiences)\n",
    "\n",
    "        # # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # # Update the weights of the q_network\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.update_target_network()\n",
    "    def compute_loss(self, experiences):\n",
    "        \"\"\"\n",
    "        Calculates the loss.\n",
    "\n",
    "        Args:\n",
    "        experiences: namedtuple with fields [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "        gamma: discount factor.\n",
    "        q_network: PyTorch model for predicting the q_values.\n",
    "        target_q_network: PyTorch model for predicting the targets.\n",
    "\n",
    "        Returns:\n",
    "        loss: Mean-Squared Error between the y targets and the Q(s,a) values.\n",
    "        \"\"\"\n",
    "        # Unpack the mini-batch of experience tuples\n",
    "        states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "        # Compute max Q^(s,a) using target network\n",
    "        with torch.no_grad():  # No gradient computation for target network\n",
    "            max_qsa = self.target_network(next_states).max(1)[0]  # max function returns both values and indices\n",
    "\n",
    "        # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a)\n",
    "        y_targets = rewards + (self.GAMMA * max_qsa * (1 - done_vals))\n",
    "\n",
    "        # Get the q_values for the actions taken\n",
    "        q_values = self.network(next_states).max(1)[0]\n",
    "\n",
    "        # Compute the loss (Mean Squared Error)\n",
    "        loss = torch.nn.functional.mse_loss(q_values, y_targets)\n",
    "        \n",
    "        # # Generate a batch of data\n",
    "        # states, actions, rewards, next_states, done_vals = experiences\n",
    "        # # Generate a batch of data\n",
    "        # states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "        # max_qsa = torch.max(self.target_network(next_states), dim=1)[0]\n",
    "        # y_targets = rewards + (self.GAMMA * max_qsa * (1 - done_vals))\n",
    "        # q_values = self.network(states)\n",
    "        # loss = torch.nn.functional.binary_cross_entropy_with_logits(q_values, y_targets.unsqueeze(1))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def update_epsilon(self, num_episodes):\n",
    "        E_DECAY = 0.995\n",
    "        # ( self.epsilon / (num_episodes / 2)\n",
    "        self.epsilon = max(self.E_MIN, self.epsilon * E_DECAY)\n",
    "        \n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for target_param, q_net_param in zip(self.target_network.parameters(), self.network.parameters()):\n",
    "            target_param.data.copy_(self.TAU * q_net_param.data + (1.0 - self.TAU) * target_param.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state size:  11\n",
      "num actions:  1\n",
      "Episode 100 | Total point average of the last 100 episodes: 48.59: epsilon = 0.6057704364907278\n",
      "Episode 200 | Total point average of the last 100 episodes: 54.96: epsilon = 0.36695782172616715\n",
      "Episode 300 | Total point average of the last 100 episodes: 66.31: epsilon = 0.22229219984074702\n",
      "Episode 400 | Total point average of the last 100 episodes: 74.84: epsilon = 0.13465804292601349\n",
      "Episode 500 | Total point average of the last 100 episodes: 83.18: epsilon = 0.08157186144027828\n",
      "Episode 600 | Total point average of the last 100 episodes: 85.89: epsilon = 0.049413822110038545\n",
      "Episode 700 | Total point average of the last 100 episodes: 88.62: epsilon = 0.029933432588273214\n",
      "Episode 800 | Total point average of the last 100 episodes: 92.06: epsilon = 0.018132788524664028\n",
      "Episode 900 | Total point average of the last 100 episodes: 93.91: epsilon = 0.010984307219379798\n",
      "Episode 1000 | Total point average of the last 100 episodes: 96.32: epsilon = 0.01036634861955105\n",
      "Episode 1100 | Total point average of the last 100 episodes: 94.40: epsilon = 0.01\n",
      "Episode 1200 | Total point average of the last 100 episodes: 93.07: epsilon = 0.01\n",
      "Episode 1300 | Total point average of the last 100 episodes: 93.73: epsilon = 0.01\n",
      "Episode 1400 | Total point average of the last 100 episodes: 95.83: epsilon = 0.01\n",
      "Episode 1500 | Total point average of the last 100 episodes: 94.08: epsilon = 0.01\n",
      "Episode 1600 | Total point average of the last 100 episodes: 94.75: epsilon = 0.01\n",
      "Episode 1700 | Total point average of the last 100 episodes: 94.93: epsilon = 0.01\n",
      "Episode 1800 | Total point average of the last 100 episodes: 93.00: epsilon = 0.01\n",
      "Episode 1900 | Total point average of the last 100 episodes: 94.05: epsilon = 0.01\n",
      "Episode 2000 | Total point average of the last 100 episodes: 99.26: epsilon = 0.01\n",
      "Episode 2100 | Total point average of the last 100 episodes: 91.98: epsilon = 0.01\n",
      "Episode 2200 | Total point average of the last 100 episodes: 96.63: epsilon = 0.01\n",
      "Episode 2300 | Total point average of the last 100 episodes: 97.62: epsilon = 0.01\n",
      "Episode 2400 | Total point average of the last 100 episodes: 93.56: epsilon = 0.01\n",
      "Episode 2500 | Total point average of the last 100 episodes: 96.10: epsilon = 0.01\n",
      "Episode 2600 | Total point average of the last 100 episodes: 95.73: epsilon = 0.01\n",
      "Episode 2700 | Total point average of the last 100 episodes: 94.87: epsilon = 0.01\n",
      "Episode 2800 | Total point average of the last 100 episodes: 95.61: epsilon = 0.01\n",
      "Episode 2900 | Total point average of the last 100 episodes: 96.04: epsilon = 0.01\n",
      "Episode 3000 | Total point average of the last 100 episodes: 96.35: epsilon = 0.01\n",
      "Episode 3100 | Total point average of the last 100 episodes: 94.19: epsilon = 0.01\n",
      "Episode 3200 | Total point average of the last 100 episodes: 91.95: epsilon = 0.01\n",
      "Episode 3300 | Total point average of the last 100 episodes: 94.11: epsilon = 0.01\n",
      "Episode 3400 | Total point average of the last 100 episodes: 94.86: epsilon = 0.01\n",
      "Episode 3500 | Total point average of the last 100 episodes: 96.57: epsilon = 0.01\n",
      "Episode 3600 | Total point average of the last 100 episodes: 94.28: epsilon = 0.01\n",
      "Episode 3700 | Total point average of the last 100 episodes: 94.74: epsilon = 0.01\n",
      "Episode 3800 | Total point average of the last 100 episodes: 92.51: epsilon = 0.01\n",
      "Episode 3900 | Total point average of the last 100 episodes: 92.44: epsilon = 0.01\n",
      "Episode 4000 | Total point average of the last 100 episodes: 93.95: epsilon = 0.01\n",
      "\n",
      "Total Runtime: 9.03 s (0.15 min)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "MAX_NUM_STEPS = 1000 #terminates after 1000 steps\n",
    "MAX_NUM_EPISODES = 4000 \n",
    "NUM_P_AVG = 100\n",
    "UPDATE_NETWORK_STEPS = 7\n",
    "\n",
    "env = gym.make('InvertedDoublePendulum-v4', render_mode=\"rgb_array\")\n",
    "STATE_SIZE = env.observation_space.shape[0]\n",
    "NUM_ACTIONS = env.action_space.shape[0]\n",
    "print(\"state size: \", STATE_SIZE)\n",
    "print(\"num actions: \", NUM_ACTIONS)\n",
    "\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "memory_buffer = deque(maxlen=10000)\n",
    "total_points_hist = []\n",
    "agent = DoublePendulumAgent(STATE_SIZE, NUM_ACTIONS)\n",
    "\n",
    "for i in range(MAX_NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    total_points = 0\n",
    "    # for t in range(MAX_NUM_STEPS):\n",
    "    done = False\n",
    "    t=0\n",
    "    while not done:\n",
    "        t+=1\n",
    "        state_qn = torch.from_numpy(np.expand_dims(state, axis=0))\n",
    "        q_values = agent.network(state_qn)\n",
    "        # print(q_values)\n",
    "        action = agent.get_action(q_values)\n",
    "        # print(action)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        if(agent.should_update_network(t, UPDATE_NETWORK_STEPS, memory_buffer)):\n",
    "            minibatch = agent.get_experiences(memory_buffer)\n",
    "            agent.agent_learn(minibatch)\n",
    "            \n",
    "        state = next_state.copy()\n",
    "        total_points += reward\n",
    "        \n",
    "    total_points_hist.append(total_points)\n",
    "    agent.update_epsilon(t)\n",
    "    \n",
    "    avg_latest_points = np.mean(total_points_hist[-NUM_P_AVG:])\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {NUM_P_AVG} episodes: {avg_latest_points:.2f}: epsilon = {agent.epsilon}\", end=\"\")\n",
    "\n",
    "    if (i+1) % NUM_P_AVG == 0:\n",
    "            print(f\"\\rEpisode {i+1} | Total point average of the last {NUM_P_AVG} episodes: {avg_latest_points:.2f}\")\n",
    "            \n",
    "    if avg_latest_points >= 300:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        torch.save(agent.network.state_dict(), 'Cart_pole_model.pth')\n",
    "        break\n",
    "    \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(filename, env, q_network, fps=30):\n",
    "    start = time.time()\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        done = False\n",
    "        state, _ = env.reset()\n",
    "        print(\"state shape: \", state)\n",
    "        frame = env.render()\n",
    "        print(\"frame shape: \", frame)\n",
    "        video.append_data(frame)\n",
    "        while not done:\n",
    "            if time.time() - start > 10:\n",
    "                break\n",
    "            state = torch.from_numpy(state)\n",
    "            print(state)\n",
    "            q_values = q_network(state)\n",
    "            print(q_values)\n",
    "            # action = np.argmax(q_values.detach().numpy()[0])\n",
    "            action = np.array([q_values.item()])\n",
    "            print(action)\n",
    "            state, _, done, _, _ = env.step(action)\n",
    "            frame = env.render()\n",
    "            video.append_data(frame)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape:  [-0.01343921  0.02334792  0.02793388  0.9997274   0.99960977  0.06306323\n",
      "  0.13887939 -0.00750996  0.          0.          0.        ]\n",
      "frame shape:  [[[78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  ...\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]]\n",
      "\n",
      " [[78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  ...\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]]\n",
      "\n",
      " [[78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  ...\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  ...\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]]\n",
      "\n",
      " [[78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  ...\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]]\n",
      "\n",
      " [[78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  ...\n",
      "  [78 88 78]\n",
      "  [78 88 78]\n",
      "  [78 88 78]]]\n",
      "tensor([-0.0134,  0.0233,  0.0279,  0.9997,  0.9996,  0.0631,  0.1389, -0.0075,\n",
      "         0.0000,  0.0000,  0.0000], dtype=torch.float64)\n",
      "tensor([32.6207], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "[32.62074467]\n",
      "tensor([ 0.0425, -0.0779,  0.1649,  0.9970,  0.9863,  2.1755, -4.1890,  5.4711,\n",
      "         0.0000,  0.0000,  0.0000], dtype=torch.float64)\n",
      "tensor([37.5319], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "[37.53188338]\n",
      "tensor([ 0.2018, -0.3719,  0.5039,  0.9283,  0.8638,  4.1344, -7.5919,  7.9833,\n",
      "         0.0000,  0.0000,  0.0000], dtype=torch.float64)\n",
      "tensor([29.0440], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "[29.04403749]\n",
      "tensor([ 0.4480, -0.7176,  0.7517,  0.6965,  0.6595,  5.6321, -8.8480,  3.9754,\n",
      "         0.0000,  0.0000,  0.0000], dtype=torch.float64)\n",
      "tensor([3.0310], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "[3.0309577]\n"
     ]
    }
   ],
   "source": [
    "create_video(\"double_pendulum.mp4\", env, agent.network)\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
